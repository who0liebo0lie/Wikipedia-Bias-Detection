# -*- coding: utf-8 -*-
"""May2025CodePudding_Fine_Prints_notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bsld6ZTWvrpgryVfTamWxLT0uYOz8fuk

# Assessing Wikipedia Bias

## Introduction

Wikipedia is one of the most widely used information platforms in the world, offering free access to knowledge on nearly every imaginable topic. One of the reasons it is so widely used is that anyone can contribute to it, and thus making it knowledge accessible, but also introduces challenges. Since so many different people contribute, it is easy for bias to slip into articles.

While Wikipedia does have policies to outright information, it is harder to catch and correct subtle bias in the way information is written. Thus, in reality language can reflect personal opinions, cultural viewpoints, or even political leanings. These influences can affect how information is presented, which might shape how readers understand a topic.

The goal of this project is to inverstigate and quantify this bias. Through a combination of data collection, exploratory data analysis (EDA), and supervised learning techniques, we aim to build a model capable of identifying biased language in Wikipedia articles - mostly at the sentence level. This will get us an overall 'bias score' for an article, offering a scalable and interpretable way to assess objectivity.

## Data Overview
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import os
import random

# models
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder
from nltk.tokenize import sent_tokenize
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.preprocessing import LabelEncoder


from wordcloud import WordCloud

import xgboost as xgb
from textblob import TextBlob

from sklearn.model_selection import GridSearchCV

from tqdm import tqdm

from transformers import BertTokenizer, BertModel
import torch
import math
import numpy as np
from tqdm import tqdm

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report

"""### Useful Functions:"""

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')

def BERT_text_to_embeddings(texts, tokenizer, bert_model, max_length=512, batch_size=100, force_device=None, disable_progress_bar=False):
    ids_list = []
    attention_mask_list = []

    for text in texts:
        encoded = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=max_length,
            padding='max_length',
            return_attention_mask=True,
            return_tensors='pt',
            truncation=True
        )
        ids_list.append(encoded['input_ids'].squeeze().tolist())
        attention_mask_list.append(encoded['attention_mask'].squeeze().tolist())

    device = torch.device(force_device) if force_device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    bert_model.to(device)

    if not disable_progress_bar:
        print(f'Using the {device} device.')

    embeddings = []

    for i in tqdm(range(math.ceil(len(ids_list) / batch_size)), disable=disable_progress_bar):
        ids_batch = torch.LongTensor(ids_list[batch_size * i:batch_size * (i + 1)]).to(device)
        attention_mask_batch = torch.LongTensor(attention_mask_list[batch_size * i:batch_size * (i + 1)]).to(device)

        with torch.no_grad():
            bert_model.eval()
            outputs = bert_model(input_ids=ids_batch, attention_mask=attention_mask_batch)

        embeddings.append(outputs.last_hidden_state[:, 0, :].detach().cpu().numpy())


    return np.concatenate(embeddings)

def train_val_test_split (df, rstate = 42, shuffle=True, stratify=None):
    if stratify != None:
        strat=df[stratify]
    else:
        strat = None
    train_set, test_set = train_test_split(df, test_size=0.4, random_state=rstate, shuffle=shuffle, stratify=strat)

    if stratify!=None:
        strat=test_set[stratify]
    else:
        strat = None
    val_set, test_set=train_test_split(test_set, test_size=0.5, random_state=rstate, shuffle=shuffle, stratify=strat)

    return (train_set, val_set, test_set)

def get_bias_score(article_text, tokenizer, bert_model, classifier_model, biased_index=0):
    sentences = sent_tokenize(article_text)
    if not sentences:
        print('No sentences in file.')
        return 0.0

    embeddings = BERT_text_to_embeddings(sentences, tokenizer, bert_model)
    preds = classifier_model.predict(embeddings)
    predicted_classes = np.argmax(preds, axis=1)

    bias_score = np.mean(predicted_classes == biased_index)
    return round(bias_score, 3)

import requests

def get_wikipedia_article(title, lang='en'):
    """
    Robustly fetches the plain text content of a Wikipedia article using the MediaWiki API.
    Supports redirects and unusual titles.
    """
    api_url = f"https://{lang}.wikipedia.org/w/api.php"
    params = {
        "action": "query",
        "format": "json",
        "prop": "extracts",
        "explaintext": True,
        "redirects": 1,
        "titles": title
    }

    response = requests.get(api_url, params=params)
    data = response.json()

    pages = data["query"]["pages"]
    page = next(iter(pages.values()))

    if "extract" in page:
        return page["extract"]
    else:
        print(f"❌ Article not found: {title}")
        return None

# Load the datasets
#try:
data = pd.read_csv('final_labels.csv', sep=';')
except:
    from google.colab import drive
    drive.mount('/content/drive')
    data = pd.read_csv('/content/drive/MyDrive/datasets/code_jam_may25/data/final_labels_SG2.csv', sep=';')

# Display the first few rows of the dataset
display(data.head())

display(data['text'])

"""### Data preprocessing"""

# Display the column names of the dataset
column_names = data.columns.tolist()
display(column_names)

# Display the shape of the dataset
n_rows, n_cols = data.shape
print(f"The DataFrame has {n_rows} rows and {n_cols} columns")

# Display the informative summary of the dataset
data.info()

# Display the descriptive statistics of the dataset

data.describe()

label_counts = data['label_bias'].value_counts()
valid_labels = label_counts[label_counts > 1].index

data = data[data['label_bias'].isin(valid_labels)]

"""## Explorating Data Analysis

### Duplicates
"""

# Display the number of duplicates in the dataset
duplicates = data[data.duplicated()]
display(f"Number of duplicated data: {duplicates.shape[0]}")

"""### Missing Values"""

# Display the number of missing values in the dataset
display(data.isna().sum())

# Check for missing values in the DataFrame as a percentage
display(data.isna().sum()/len(data))

# Drop rows with missing values in the 'news_link' and 'article' columns
data.dropna(subset=['news_link'], inplace=True)
data.dropna(subset=['type'], inplace=True)

# Check for missing values in the DataFrame as a percentage
display(data.isna().sum()/len(data))

# Cleaning the text data in the 'text' column
# Define a function to clean the text data
def clear_text(text):
    text = text.lower()
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"[^a-z\s]","", text)
    text = text.split()
    return " ".join(text)

data['label_bias'] = data['label_bias'].str.lower()
data['topic'] = data['topic'].str.replace('-', ' ').str.lower()

data['biased_words'] = data['biased_words'].str.replace(r"[\[\]']", "", regex=True).str.replace(",", " ").str.strip()

data.columns

# Apply the clear_text function to the 'comment_text' column
data['clean_text'] = data['text'].astype(str).apply(clear_text)
data= data.drop(columns=['text'])

# Display the first 5 rows of the comments DataFrame after cleaning
display(data.sample(5))

# Check for missing values
print(data['clean_text'].isna().sum())

data.info()

>>> import nltk
  >>> nltk.download('stopwords')

## Set of English stop words
stop_words =  set(stopwords.words('english'))

# Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()

def lemmatize(text):
    tokens = word_tokenize(text.lower())
    tokens = [token for token in tokens if token not in stop_words]
    lemmas = [lemmatizer.lemmatize(token) for token in tokens]
    return " ".join(lemmas)

>>> import nltk
  >>> nltk.download('punkt_tab')


  >>> import nltk
  >>> nltk.download('wordnet')

# Apply the clear_text function to the 'comment_text' column
data['lemmatize_text'] = data['clean_text'].apply(lemmatize)

# Display the first rows of the comments DataFrame after cleaning
display(data[['clean_text', 'lemmatize_text']].head(20))

# Added a new column to convert 'biased' to 1 and 'non-biased' to 0
data['is_biased'] = data['label_bias'].apply(lambda x: 1 if x == 'biased' else 0)

# Making sure the new column 'is_biased' is present in the dataset
data.head()

# Display the shape of the dataset after completing EDA.
n_rows, n_cols = data.shape
print(f"The DataFrame has {n_rows} rows and {n_cols} columns")

"""### Conclusion: Data Preprocessing and EDA

In this part, we conducted a thorough preprocessing and exploratory data analysis (EDA) to prepare the dataset for bias detection modeling. Firstly, we reviewed the dataset's structure, identifying the number of rows and columns, and inspecting its basic information and summary statistics. We then addressed data quality issues, including duplicates, and handling missing values. Doing this, there were no duplicates found in the data, while missing values were found in two columns 'news_link' and 'type' in negligible percentage of the dataset. Thus, rows will missing values were dropped.

Next, we focused on cleaning and standarizing the text data. This included lowercasing, removing URLs, special characters, and stopwords, as well as lemmatizing words to reduce them to their root forms. These steps help normalize the text and reduce noise, which is essential for any NLP-based modeling.

To better analyze bias, we created a binary label column ('is_biased') to distinguish between biased and unbiased texts. This will serve as our target variable in supervised which will be used as input features for further linguistic anaylsis and model training.

Overall, the dataset is now well-structured, cleaned, and ready for feature extraction, model building, and further investigation into biased language patterns.

#### Which topics have the most biased words on average per article?
"""

bias_by_topic = data.groupby('topic')['is_biased'].mean().sort_values(ascending=False).head(10)
bias_by_topic.plot(kind='barh')
plt.title('Average Bias Rate by Topic')
plt.xlabel('Proportion of Biased Sentences')
plt.show()

"""The graph above reveals that certain topics attract more biased language than others. 'Trump presidency' and 'white nationalism' have the highest average bias rates, with over 70% of sentences labeled as biased. These findings suggest that politically and socially charged topics are more prone to emotionally loaded language. On the other hand, topics like 'immigration', 'universal healthercare', and 'gun control' show lower bias rates, though they still remain near 45-50%, indicating that even these texts are not free from bias.

Understanding which topics are more likely to contain biased language helps contextualize the limitations of people-contributed platforms like Wikipedia and can inform targeted moderation or review efforts.

#### Is the dataset balanced between biased and unbiased data rows?
"""

sns.countplot(x='is_biased', data=data)
plt.title('Class Distribution of Bias')
plt.xticks([0,1], ['Unbiased (0)', 'Biased (1)'])
plt.show()

"""The graph indicates that the dataset is nearly balanced, with a slightly higher number of unbiased (0) data rows compared to biased (1). This is ideal for supervising learning, as a balanced dataset helps prevent the model from becoming biased toward the majority class. It increases the likelihood of accurate predictions for both biased and unbiased text during model training and evaluation.

#### Which words are most frequently flagged as biased across the dataset?
"""

all_biased_words = data['biased_words'].dropna().astype(str).str.split()

all_biased_words = [word for sublist in all_biased_words for word in sublist]

top_words = pd.Series(all_biased_words).value_counts().head(15)

excluded_words = ['the', 'of']

# Count top words, then filter out the excluded ones
top_words = (
    pd.Series(all_biased_words)
    .value_counts()
    .drop(labels=excluded_words, errors='ignore')  # drop excluded
    .head(15)
)

# Plot
top_words.plot(kind='barh', figsize=(10,6), color='slateblue')
plt.xlabel('Frequency')
plt.title('Top 15 Most Frequently Flagged Biased Words')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

top_words.head(25)

"""#### How does sentiment differ between biased and unbiased texts?

MRCP_Concereteness_Ratings is a rating of  tangiblility of a word.
MRCP_Imagability_Ratings is how easily a word evokes a mental image.

Overall these ideas are connected so that:
Low concreteness + high imagability can suggest emotional or persuasive bias.
High concreteness + low imagability often points to factual reporting.

#### What proportion of an article's sentences are labeled as biased or opinionated?
"""

data['sentiment'] = data['clean_text'].apply(lambda x: TextBlob(x).sentiment.polarity)

plt.figure(figsize=(10, 6))
sns.boxplot(x='is_biased', y='sentiment', data=data)
plt.title('Sentiment Polarity by Bias Label')
plt.xlabel('Is Biased (1 = Yes, 0 = No)')
plt.ylabel('Sentiment Polarity')
plt.xticks([0, 1], ['Unbiased', 'Biased'])
plt.show()

"""The graph above analyzes the sentiment polarity across biased and unbiased text of different articles. It reveals that while both categories generally maintain fa neutral tone on average, biased articles tend to exhibit a wider range of sentiment. This includes a higher frequency of strongly positive or negative sentiment, indicating that biased writing often uses more emotionally charge language. In contrast, unbiased texts shows a tighter sentiment distribution, reflecting a more balanced and objective tone. This suggests that sentiment polarity can serve as a useful signal in detecting bias, particularly when combined with other linguistic and contextual features.

#### Which news outlet have the highest average bias score?
"""

avg_bias_by_outlet = data.groupby('outlet')['is_biased'].mean().sort_values(ascending=False).reset_index()

# Display top outlets by average bias score
print(avg_bias_by_outlet.head())

plt.figure(figsize=(12,6))
sns.barplot(data=avg_bias_by_outlet, x='outlet', y='is_biased', palette='Reds_r')
plt.xticks(rotation=45, ha='right')
plt.ylabel('Average Bias Score')
plt.title('Average Bias Score by News Outlet')
plt.tight_layout()
plt.show()

"""The bar graph displays the average proportion of biased sentences per article across different news outlets. The data suggests that media outlets vary widely in how frequently they use biased language. Alternet and Federalist have the highest average bias scores, suggesting strong ideological framing. Breitbart and HuffPost also show high bias, while MSNBC and Fox News despite opposing views - have similar moderate scores.Reuters and USA Today stand out for their low bias, reflecting more neutral reporting. Overall, outlets with a strong ideological orientation tend to have higher bias scores, whereas mainstream or centrist outlets tend to score lower. In particular, Reuters stands out for having the most neutral tone in its reporting.

## Modeling
"""

# Before modeling, checking the total counts for 'biased' and 'non-biased'.
data['is_biased'].value_counts()

data.columns

# Only keeping input and output
df = data[['clean_text', 'label_bias']]

le = LabelEncoder()
df['label_encoded'] = le.fit_transform(df['label_bias'])

df.columns

# Splitting in train and test (from Tirso)
# Remove stratification if a class has only one instance
if df['label_encoded'].nunique() > 1 and df['label_encoded'].value_counts().min() >= 2:
    train_set, val_set, test_set = train_val_test_split(df, stratify='label_encoded')
else:
    # If stratification is not possible, perform a regular split
    train_set, val_set, test_set = train_val_test_split(df)

xtrain, ytrain = train_set['clean_text'], train_set['label_encoded']
xval, yval = val_set['clean_text'], val_set['label_encoded']
xtest, ytest = test_set['clean_text'], test_set['label_encoded']

# Splitting in train and test (from Tirso)
#train_set, val_set, test_set = train_val_test_split(df, stratify='label_encoded')
#xtrain, ytrain = train_set['clean_text'], train_set['label_encoded']
#xval, yval = val_set['clean_text'], val_set['label_encoded']
#xtest, ytest = test_set['clean_text'], test_set['label_encoded']

"""### BERT Model:"""

xtrain

embeddings_train = BERT_text_to_embeddings(xtrain, tokenizer, bert_model)
embeddings_val = BERT_text_to_embeddings(xval, tokenizer, bert_model)
embeddings_test = BERT_text_to_embeddings(xtest, tokenizer, bert_model)

print(le.classes_)
print(type(le.classes_[0]))

"""### TF-IDF & Logistic Regression:"""

X_train, X_test, y_train, y_test = train_test_split(
    data['lemmatize_text'], data['is_biased'], test_size=0.2, random_state=42
)

vectorizer = TfidfVectorizer(max_features=5000) # Initialize the TfidfVecotrizer
X_train_vec = vectorizer.fit_transform(X_train) # Fit and transform the training data
X_test_vec = vectorizer.transform(X_test)# Transform the test data

X_train.shape

X_train_vec.shape

y_train.shape

model_lr = LogisticRegression()
model_lr.fit(X_train_vec, y_train)

y_pred = model_lr.predict(X_test_vec)

print("Accuracy:", accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
}

model_rf = RandomForestClassifier(random_state=7)

grid_search = GridSearchCV(
    estimator=model_rf,
    param_grid=param_grid,
    cv=5,
    scoring='f1',
    verbose=2,
)

grid_search.fit(X_train_vec, y_train)

print("Best Parameters:", grid_search.best_params_)

best_rf = grid_search.best_estimator_
y_pred = best_rf.predict(X_test_vec)

print("Accuracy:", accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

param_grid = {
    'n_estimators': [50, 100,],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 4, 5],
    'subsample': [0.7, 0.8, 1.0],
    'colsample_bytree': [0.7, 0.8, 1.0],
    'scale_pos_weight': [1, 2, 3],
}

"""## XGB Classifier:"""

model_xg = XGBClassifier(eval_metric='logloss')

grid_search = GridSearchCV(estimator=model_xg, param_grid=param_grid,
                           scoring='f1', cv=3, verbose=1, n_jobs=-1)

grid_search.fit(X_train_vec, y_train)

print("Best Hyperparameters:", grid_search.best_params_)

best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test_vec)

print("Accuracy:", accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""## LGBM Classifier:"""

param_grid = {
    'num_leaves': [15, 31, 63],
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [100],
    'max_depth': [-1, 10, 20],
}

model_lg = LGBMClassifier(eval_metric='logloss')

grid_search = GridSearchCV(estimator=model_lg, param_grid=param_grid,
                           scoring='f1', cv=3, verbose=1)

grid_search.fit(X_train_vec, y_train)

print("Best Hyperparameters:", grid_search.best_params_)

best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test_vec)

print("Accuracy:", accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""## Neural Network with Keras"""

ytrain_nn = ytrain.values
yval_nn = yval.values
ytest_nn = ytest.values

model = Sequential([
    Dense(512, activation='gelu', input_shape=(embeddings_train.shape[1],)),
    Dropout(0.3),
    Dense(256, activation='gelu'),
    Dropout(0.3),
    Dense(len(set(ytrain)), activation='softmax')
])


model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history = model.fit(
    embeddings_train, ytrain,
    validation_data=(embeddings_val, yval),
    epochs=60,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

ypred = model.predict(embeddings_test).argmax(axis=1)
ConfusionMatrixDisplay.from_predictions(ytest, ypred, display_labels=le.classes_)

model_results = {
    'Model Name': ['TF-IDF Logistic Regression', 'Random Forest Classifier', 'XGB Classifier', 'LightGBM Classifier', 'Neural Network'],
    'Accuracy': [0.718, 0.671, 0.642, 0.682, 0.746]
}

df_models = pd.DataFrame(model_results)
df_models

#plot results
plt.figure(figsize=(10, 6))
sns.barplot(x='Model Name', y='Accuracy', data=df_models)
plt.xticks(rotation=45, ha='right')
plt.title('Model Comparison: Accuracy Scores')
plt.ylabel('Accuracy')
plt.xlabel('Model Name')
plt.tight_layout()
plt.show()

"""The neural network is the best performing one so this will be used.

## 4. You need to have a prediction function that can take in a new wikipedia article and predict how biased it is. You can do this by predicting if each sentence in an article is biased, then perhaps scaling the results by the length of the article to get somewhat of a “bias score”
"""

article_text = get_wikipedia_article("2024 United States presidential election")

print(article_text)

"""### Neural Network + BERT on Chosen Article"""

bias_score = get_bias_score(
    article_text=article_text,
    tokenizer=tokenizer,
    bert_model=bert_model,
    classifier_model=model,
    biased_index=0
)

print(f"The bias score for this article is {bias_score}")

def get_bias_score(article_text, bert_model, classifier_model, le):
    """
    Predicts how biased a Wikipedia article is using a BERT + Keras classifier.
    Returns a float between 0 and 1.
    """
    sentences = sent_tokenize(article_text)
    if not sentences:
        return 0.0

    # Pass tokenizer and bert_model to BERT_text_to_embeddings
    embeddings = BERT_text_to_embeddings(sentences, tokenizer=tokenizer, bert_model=bert_model)
    preds = classifier_model.predict(embeddings)
    predicted_classes = np.argmax(preds, axis=1)

    # Get the integer value that corresponds to 'biased'
    biased_index = np.where(le.classes_ == 'biased')[0][0]
    bias_score = np.mean(predicted_classes == biased_index)

    return round(bias_score, 3)

import requests

def get_wikipedia_article(title, lang='en'):
    """
    Robustly fetches the plain text content of a Wikipedia article using the MediaWiki API.
    Supports redirects and unusual titles.
    """
    api_url = f"https://{lang}.wikipedia.org/w/api.php"
    params = {
        "action": "query",
        "format": "json",
        "prop": "extracts",
        "explaintext": True,
        "redirects": 1,
        "titles": title
    }

    response = requests.get(api_url, params=params)
    data = response.json()

    pages = data["query"]["pages"]
    page = next(iter(pages.values()))

    if "extract" in page:
        print(f"✅ Article found: {title}")
        return page["extract"]
    else:
        print(f"❌ Article not found: {title}")
        return None

sample_article = """
The politician claimed the tax cut would help the middle class, but experts say it overwhelmingly benefits the wealthy.
Critics have called this policy a disaster, citing numerous studies.
Meanwhile, supporters insist the economy has improved as a result.
"""

bias_score = get_bias_score(sample_article, bert_model=bert_model, classifier_model=model, le=le)
print("Bias Score:", bias_score)

print("Label mapping:", list(le.classes_))

article_text = get_wikipedia_article("2024 United States presidential election")

print(article_text)

if article_text:
   bias_score = get_bias_score(article_text, bert_model=bert_model, classifier_model=model, le=le) # Corrected argument order
   print("Bias score:", bias_score)

lemmatizer = WordNetLemmatizer()

def clean_and_lemmatize(text):
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = text.lower()
    tokens = text.split()
    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]
    return ' '.join(lemmatized)

sentences = sent_tokenize(article_text)
cleaned_sentences = [clean_and_lemmatize(s) for s in sentences]

display(cleaned_sentences)

vectorizer = TfidfVectorizer(max_features=5000)
# Fit the vectorizer to training data (e.g., cleaned_sentences)
vectorizer.fit(cleaned_sentences) # this line is added to fit the vectorizer
x_article_vec = vectorizer.transform(cleaned_sentences) #then we can transform the data

# Get BERT embeddings for the sentences
embeddings_article = BERT_text_to_embeddings(cleaned_sentences, tokenizer=tokenizer, bert_model=bert_model) # Pass tokenizer and bert_model explicitly

# Make predictions using the Keras model
predictions = model.predict(embeddings_article)

# Calculate bias score
bias_score = predictions.sum() / len(predictions)


# Get BERT embeddings for the sentences
#embeddings_article = BERT_text_to_embeddings(cleaned_sentences)

# Make predictions using the Keras model
#predictions = model.predict(embeddings_article)

# Calculate bias score
#bias_score = predictions.sum() / len(predictions)
#change 2
#predictions = model.predict(x_article_vec)

bias_score = predictions.sum() / len(predictions)

df_sent = pd.Series(sentences, name='sentence')
df_clean = pd.Series(cleaned_sentences, name='cleaned')
# Extract the predicted probabilities for the 'biased' class (assuming it's the second class)
df_pred = pd.Series(predictions[:, 1], name='is_biased')
results_df = pd.concat([df_sent, df_clean, df_pred], axis=1)

bias_score = results_df['is_biased'].sum() / len(results_df)

bias_score = round(bias_score, 2)

print(f"Bias Score for this article: {bias_score}")

#Bubble chart showing article distribution across bias types

#results_df is  DataFrame with 'sentence', 'cleaned', and 'is_biased' columns
plt.figure(figsize=(10, 6))
plt.scatter(results_df.index, results_df['is_biased'], s=results_df['is_biased'] * 100, alpha=1)
plt.xlabel('Sentence Index')
plt.ylabel('Bias Probability')
plt.title('Neural Network Bias Probability Distribution across Sentences')
plt.grid(True)
plt.show()

"""### TF-IDF Logisitic Regression of Chosen Article"""

lemmatizer = WordNetLemmatizer()

def clean_and_lemmatize(text):
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = text.lower()
    tokens = text.split()
    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]
    return ' '.join(lemmatized)

sentences = sent_tokenize(article_text)
cleaned_sentences = [clean_and_lemmatize(s) for s in sentences]

display(cleaned_sentences)

#Fit Vectorizer on Training Data BEFORE training the LogisticRegression model
vectorizer = TfidfVectorizer(max_features=5000)
vectorizer.fit(data['lemmatize_text']) # Fit to your entire dataset's preprocessed text

# Assuming you had previously split your data as follows
X_train, X_test, y_train, y_test = train_test_split(data['lemmatize_text'], data['is_biased'], test_size=0.2, random_state=42)

# Vectorize the train and test data using the fitted vectorizer
X_train_vec = vectorizer.transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# Now train your LogisticRegression model
model_lr = LogisticRegression()
model_lr.fit(X_train_vec, y_train)

x_article_vec = vectorizer.transform(cleaned_sentences)  # Use the same 'vectorizer' instance

predictions = model_lr.predict(x_article_vec)

bias_score = predictions.sum() / len(predictions)

df_sent = pd.Series(sentences, name='sentence')
df_clean = pd.Series(cleaned_sentences, name='cleaned')
df_pred = pd.Series(predictions, name='is_biased')

results_df = pd.concat([df_sent, df_clean, df_pred], axis=1)

bias_score = results_df['is_biased'].sum() / len(results_df)

bias_score = round(bias_score, 2)

print(f"Bias Score for this article: {bias_score}")

#Bubble chart showing article distribution across bias types

#results_df is  DataFrame with 'sentence', 'cleaned', and 'is_biased' columns
plt.figure(figsize=(10, 6))
plt.scatter(results_df.index, results_df['is_biased'], s=results_df['is_biased'] * 100, alpha=1, color='red')
plt.xlabel('Sentence Index')
plt.ylabel('Bias Probability')
plt.title('Logistic Regression Bias Probability Distribution across Sentences')
plt.grid(True)
plt.show()

"""## Conclusion

Our analysis reveals that certain topics consistently attract more biased language than others. Notably, articles related to the Trump presidency and white nationalism show the highest average bias, with over 70% of sentences labeled as biased. This suggests that politically and socially charged subjects tend to include more emotionally loaded or opinionated language.

In contrast, topics like immigration, universal healthcare, and gun control exhibit lower—but still significant—bias levels, typically around 45–50%. This highlights that even seemingly neutral or policy-driven discussions on Wikipedia are not immune to subtle bias.

We also analyzed sentiment polarity across biased and unbiased texts. While both categories generally maintain a neutral tone, biased articles show a wider emotional range—often skewing strongly positive or negative. Unbiased texts, by contrast, reflect a tighter, more balanced distribution. This suggests that sentiment polarity may serve as a useful feature in bias detection, especially when combined with other linguistic markers.

Common biased word patterns include variations of “claim” (e.g., claims, claimed, claiming), as well as emotionally charged terms like “illegal” and “aliens.”

Our findings further show that media sources differ significantly in their bias levels. Outlets like Alternet and The Federalist ranked highest in bias, followed by Breitbart and HuffPost. Interestingly, both Fox News and MSNBC—despite differing political leanings—had similar moderate bias levels. Reuters and USA Today stood out as the most neutral, underscoring the influence of editorial standards on perceived bias.

We trained several machine learning models to predict bias, with the following results (F1 score):

Neural Network: 0.746

TF-IDF + Logistic Regression: 0.718

LightGBM Classifier: 0.682

Random Forest Classifier: 0.671

XGBoost Classifier: 0.642

A case study on a randomized Wikipedia article about the 2024 U.S. presidential election illustrated these patterns. While labeled as “center,” the article contained emotionally weighted language, showing that even supposedly neutral entries can exhibit subtle bias. Neural network article bias is 0.64.  TF-IDF Logistic Regression bias is 0.54.

These results underscore the importance of critical reading and the potential for automated tools to assist in bias detection and editorial review on open platforms like Wikipedia.
"""